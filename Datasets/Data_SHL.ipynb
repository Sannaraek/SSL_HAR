{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if running on googlecolab \n",
    "# !pip install hickle\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/')\n",
    "# %cd drive/MyDrive/PerCom2021-FL-master/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hickle as hkl \n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from subprocess import call\n",
    "import requests \n",
    "np.random.seed(0)\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for loading and downloading the dataset\n",
    "\n",
    "# load a single file as a numpy array\n",
    "def load_file(filepath):\n",
    "\tdataframe = pd.read_csv(filepath, header=None, delim_whitespace=True)\n",
    "\treturn dataframe.values\n",
    " \n",
    "# load a list of files, such as x, y, z data for a given variable\n",
    "def load_group(filenames, prefix=''):\n",
    "\tloaded = list()\n",
    "\tfor name in filenames:\n",
    "\t\tdata = load_file(prefix + name)\n",
    "\t\tloaded.append(data)\n",
    "\t# stack group so that features are the 3rd dimension\n",
    "\tloaded = np.dstack(loaded)\n",
    "\treturn loaded\n",
    " \n",
    "# load a dataset group, such as train or test\n",
    "def load_dataset(group, prefix='',position=''):\n",
    "\tfilepath = prefix + '/' + group + '/' + position\n",
    "\tfilenames = list()\n",
    "\t# body acceleration\n",
    "\tfilenames += ['Acc_x.txt', 'Acc_y.txt', 'Acc_z.txt']\n",
    "\t# body gyroscope\n",
    "\tfilenames += ['Gyr_x.txt', 'Gyr_y.txt', 'Gyr_z.txt']\n",
    "\t# load input data\n",
    "\tx = np.asarray(load_group(filenames, filepath))\n",
    "\t# load class output\n",
    "\ty =  processLabel(load_file(filepath+'/Label.txt'))\n",
    "\treturn x, y\n",
    "\n",
    "# download function for datasets\n",
    "def download_url(url, save_path, chunk_size=128):\n",
    "    r = requests.get(url, stream=True)\n",
    "    with open(save_path, 'wb') as fd:\n",
    "        for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "            fd.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = [\n",
    "        \"SHLDataset_preview_v1_part1.zip\",\n",
    "        \"SHLDataset_preview_v1_part2.zip\",\n",
    "        \"SHLDataset_preview_v1_part3.zip\"\n",
    "           ]\n",
    "links = [\n",
    "    \"http://www.shl-dataset.org/wp-content/uploads/SHLDataset_preview_v1_part1.zip\",\n",
    "        \"http://www.shl-dataset.org/wp-content/uploads/SHLDataset_preview_v1_part2.zip\",\n",
    "        \"http://www.shl-dataset.org/wp-content/uploads/SHLDataset_preview_v1_part3.zip\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHLDataset_preview_v1_part1.zip already downloaded\n",
      "SHLDataset_preview_v1_part2.zip already downloaded\n",
      "SHLDataset_preview_v1_part3.zip already downloaded\n"
     ]
    }
   ],
   "source": [
    "# download and unzipping dataset/download\n",
    "os.makedirs('dataset/download',exist_ok=True)\n",
    "os.makedirs('dataset/extracted',exist_ok=True)\n",
    "\n",
    "for i in range(len(fileName)):\n",
    "    data_directory = os.path.abspath(\"dataset/download/\"+str(fileName[i]))\n",
    "    if not os.path.exists(data_directory):\n",
    "        print(\"downloading \"+str(fileName[i]))            \n",
    "        download_url(links[i],data_directory)\n",
    "        print(\"download done\")\n",
    "        data_directory2 =  os.path.abspath(\"dataset/extracted/\"+str(fileName[i]))\n",
    "        print(\"extracting data...\")\n",
    "        with zipfile.ZipFile(data_directory, 'r') as zip_ref:\n",
    "            zip_ref.extractall(os.path.abspath(\"dataset/extracted/\"))\n",
    "        print(\"data extracted\")\n",
    "    else:\n",
    "        print(str(fileName[i]) + \" already downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findRanges(nums):\n",
    "    nums = sorted(set(nums))\n",
    "    gaps = [[s, e] for s, e in zip(nums, nums[1:]) if s+1 < e]\n",
    "    edges = iter(nums[:1] + sum(gaps, []) + nums[-1:])\n",
    "    return list(zip(edges, edges))\n",
    "\n",
    "def unionRange(a):\n",
    "    b = []\n",
    "    for begin,end in sorted(a):\n",
    "        if b and b[-1][1] >= begin - 1:\n",
    "            b[-1][1] = max(b[-1][1], end + 1)\n",
    "        else:\n",
    "            b.append([begin, end + 1])\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bodyLocations = [\"Bag\",\"Hand\",\"Hips\",\"Torso\"] \n",
    "rootDirectory = 'dataset/extracted/SHLDataset_preview_v1'\n",
    "# if os.path.exists(rootDirectory+\"/scripts\"):\n",
    "#     os.remove(rootDirectory+\"/scripts\")\n",
    "dirs = [d for d in os.listdir(rootDirectory) if os.path.isdir(os.path.join(rootDirectory, d))]\n",
    "dirs.remove(\"scripts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processLabel(labels):\n",
    "    uniqueCount = np.unique(labels,return_counts=True)\n",
    "    if(len(uniqueCount[0]) > 1):\n",
    "        return uniqueCount[0][np.argmax(uniqueCount[1])]\n",
    "    else:\n",
    "        return uniqueCount[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentData(accData,time_step,step):\n",
    "#     print(accData.shape)\n",
    "    segmentAccData = list()\n",
    "    sliceIndex = np.arange(0, 256, 2)\n",
    "    for i in range(0, accData.shape[0] - time_step,step):\n",
    "#         dataSlice = accData[i:i+time_step,:]\n",
    "#         dataSlice = np.delete(dataSlice,sliceIndex,  0)\n",
    "#         segmentAccData.append(dataSlice)\n",
    "#         segmentAccData.append(signal.decimate(accData[i:i+time_step,:],2))\n",
    "        segmentAccData.append(accData[i:i+time_step,:])\n",
    "\n",
    "\n",
    "    return np.asarray(segmentAccData)\n",
    "def segmentLabel(accData,time_step,step):\n",
    "#     print(accData.shape)\n",
    "    segmentAccData = list()\n",
    "    for i in range(0, accData.shape[0] - time_step,step):\n",
    "        segmentAccData.append(processLabel(accData[i:i+time_step]))\n",
    "        \n",
    "    return np.asarray(segmentAccData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# userData = {new_list: [] for new_list in range(4)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downSampleLowPass(motionData):\n",
    "    accX = signal.decimate(motionData[:,:,0],2)\n",
    "    accY = signal.decimate(motionData[:,:,1],2)\n",
    "    accZ = signal.decimate(motionData[:,:,2],2)\n",
    "    gyroX = signal.decimate(motionData[:,:,3],2)\n",
    "    gyroY = signal.decimate(motionData[:,:,4],2)\n",
    "    gyroZ = signal.decimate(motionData[:,:,5],2)\n",
    "    return np.dstack((accX,accY,accZ,gyroX,gyroY,gyroZ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data...\n",
      "procesing User1\n",
      "procesing User2\n",
      "procesing User3\n"
     ]
    }
   ],
   "source": [
    "print(\"processing data...\")\n",
    "userData = []\n",
    "userLabel = []\n",
    "for userSubFolder in dirs:\n",
    "    print(\"procesing \"+userSubFolder)\n",
    "    subDir =  rootDirectory + \"/\"+userSubFolder\n",
    "    timeSubFolder = [d for d in os.listdir(subDir) if os.path.isdir(os.path.join(subDir, d))]\n",
    "\n",
    "    start = True\n",
    "    for timeDir in timeSubFolder:\n",
    "        dataPosition = []\n",
    "        dataDir = subDir + \"/\" + timeDir\n",
    "        labelPosition = load_file(dataDir+\"/Label.txt\")[:,1]\n",
    "        nanSortedLocation = []\n",
    "        for locations in bodyLocations:\n",
    "            dataPosition.append(load_file(dataDir+\"/\"+locations +\"_Motion.txt\")[:,1:7])\n",
    "            nanlocation = findRanges(np.unique(np.where(np.isnan(dataPosition[-1]))))\n",
    "            for eachRange in nanlocation:\n",
    "                nanSortedLocation.append(eachRange)\n",
    "        deleteRange = unionRange(nanSortedLocation)\n",
    "        for i in reversed(range(len(deleteRange))):\n",
    "            labelPosition = np.delete(labelPosition,np.s_[deleteRange[i][0]:deleteRange[i][1]],axis=0)\n",
    "            for bodyCount in range(len(bodyLocations)):\n",
    "                dataPosition[bodyCount]  = np.delete(dataPosition[bodyCount],np.s_[deleteRange[i][0]:deleteRange[i][1]],axis=0)\n",
    "                \n",
    "                \n",
    "#         segmenting data and removing null class frames       \n",
    "        labelPosition = segmentLabel(labelPosition,256,128) \n",
    "        nullFrameToDelete = np.where(labelPosition == 0 )\n",
    "        labelPosition = np.delete(labelPosition,nullFrameToDelete)\n",
    "        labelPosition = labelPosition - 1\n",
    "#         labelPosition = np.concatenate((labelPosition, np.tile(labelPosition,len(bodyLocations)-1)))\n",
    "        labelPosition = np.swapaxes(np.repeat(labelPosition[:,  np.newaxis], len(bodyLocations), axis=1),0,1)\n",
    "        for bodyCount in range(len(bodyLocations)):\n",
    "            dataPosition[bodyCount] = segmentData(dataPosition[bodyCount],256,128)\n",
    "            dataPosition[bodyCount] = np.delete(dataPosition[bodyCount],nullFrameToDelete,axis=0)\n",
    "            dataPosition[bodyCount] = downSampleLowPass(dataPosition[bodyCount])\n",
    "        userData.append(dataPosition)\n",
    "        userLabel.append(labelPosition)\n",
    "#     for bodyCount in range(len(bodyLocations)):\n",
    "#         userData.append((np.vstack((userData[bodyCount]))))\n",
    "#         userLabel.append((np.hstack((userLabel[bodyCount]))))\n",
    "#         userData.append(np.vstack(dataPosition))\n",
    "#         userLabel.append(labelPosition)\n",
    "        \n",
    "        \n",
    "#     for bodyCount in range(len(bodyLocations)):\n",
    "#         userLabel[bodyCount] = userLabel\n",
    "#     userLabel.append(np.hstack(userLabel))\n",
    "#     userData.append(np.vstack(userData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedUserData = np.hstack((userData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedAccData = combinedUserData[:,:,:,:3]\n",
    "combinedGyroData = combinedUserData[:,:,:,3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "accMean =  np.mean(combinedAccData)\n",
    "accStd =  np.std(combinedAccData)\n",
    "                   \n",
    "gyroMean =  np.mean(combinedGyroData)\n",
    "gyroStd =  np.std(combinedGyroData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "userCount = len(userData)\n",
    "bodyCount = len(userData[0])\n",
    "labels = []\n",
    "flattenedData = []\n",
    "\n",
    "for clientIndex in range(userCount):\n",
    "    for bodyIndex in range(bodyCount):\n",
    "        userData[clientIndex][bodyIndex][:,:,:3] = (userData[clientIndex][bodyIndex][:,:,:3] - accMean)/accStd\n",
    "        userData[clientIndex][bodyIndex][:,:,3:] = (userData[clientIndex][bodyIndex][:,:,3:] - gyroMean)/gyroStd\n",
    "        labels.append(userLabel[clientIndex][bodyIndex])\n",
    "        flattenedData.append(userData[clientIndex][bodyIndex])\n",
    "        # userData = np.vstack((userData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattenedData = np.asarray(flattenedData, dtype=\"object\")\n",
    "labels = np.asarray(labels, dtype=\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataName = 'SHL'\n",
    "os.makedirs('datasetClientsUnion/'+dataName, exist_ok=True)\n",
    "hkl.dump(flattenedData,'datasetClientsUnion/'+dataName+ '/clientsData.hkl' )\n",
    "hkl.dump(labels,'datasetClientsUnion/'+dataName+ '/clientsLabel.hkl' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data processing finished\n"
     ]
    }
   ],
   "source": [
    "print(\"data processing finished\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
